<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>In all fairness: Engineering Fairness in Modern Machine Learning Algorithms - My Mind Closet</title><meta name="Description" content="A place where I stow my thoughts on a variety of topics"><meta property="og:title" content="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms" />
<meta property="og:description" content="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms I was a young bright eyed MS student at NeurIPS 2017 taking it all in - CNNs were still the rage, LeCun was still a celebrity and the air was filled with possibilities of these deep neural networks addressing problems that were thought to be pretty difficult up until now (unlike today where everything is some version of LLM applied to x-problem sigh)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-07-23T14:45:00+00:00" />
<meta property="article:modified_time" content="2025-07-23T14:45:00+00:00" /><meta property="og:site_name" content="My Mind Closet" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms"/>
<meta name="twitter:description" content="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms I was a young bright eyed MS student at NeurIPS 2017 taking it all in - CNNs were still the rage, LeCun was still a celebrity and the air was filled with possibilities of these deep neural networks addressing problems that were thought to be pretty difficult up until now (unlike today where everything is some version of LLM applied to x-problem sigh)."/>
<meta name="application-name" content="My cool site">
<meta name="apple-mobile-web-app-title" content="My cool site"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/" /><link rel="prev" href="https://aicaffeinelife.github.io/posts/2024_06_07_classical_shadows/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "In all fairness: Engineering Fairness in Modern Machine Learning Algorithms",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/aicaffeinelife.github.io\/posts\/2025_07_23_fairlearn\/"
        },"genre": "posts","wordcount":  2325 ,
        "url": "https:\/\/aicaffeinelife.github.io\/posts\/2025_07_23_fairlearn\/","datePublished": "2025-07-23T14:45:00+00:00","dateModified": "2025-07-23T14:45:00+00:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Ankit Kulshrestha"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="My Mind Closet">My Mind Closet</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/about"> About </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="My Mind Closet">My Mind Closet</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/about" title="">About</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">In all fairness: Engineering Fairness in Modern Machine Learning Algorithms</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="akulsh912.com" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Ankit Kulshrestha</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2025-07-23">2025-07-23</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;2325 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;11 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#the-problem-setup">The Problem Setup</a></li>
    <li><a href="#from-insights-to-algorithmic-components">From insights to algorithmic components</a></li>
    <li><a href="#testing-the-insights-on-real-data">Testing the insights on real data</a></li>
    <li><a href="#measures-of-fairness">Measures of Fairness</a></li>
    <li><a href="#modeling-constraints-within-machine-learning-algorithms">Modeling constraints within machine learning algorithms</a></li>
    <li><a href="#how-much-fairness-is-enough">How much fairness is enough?</a></li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="in-all-fairness-engineering-fairness-in-modern-machine-learning-algorithms">In all fairness: Engineering Fairness in Modern Machine Learning Algorithms</h1>
<p>I was a young bright eyed MS student at NeurIPS 2017 taking it all in - CNNs were still the rage, LeCun was still a celebrity and the air was filled with possibilities of these deep neural networks addressing problems that were thought to be pretty difficult up until now (unlike today where everything is some version of LLM applied to x-problem <em>sigh</em>). Engineers often focus on making a problem work and often overlook a crucial factor in engineering - it has to interact with people in the <em>real world</em>. It was at this conference that I had the good fortune of listening to Kate Crawford&rsquo;s <a href="https://www.youtube.com/watch?v=fMym_BKWQzk&amp;" target="_blank" rel="noopener noreffer ">keynote</a> on the inherent bias in machine learning algorithms.  I found it especially interesting since many of the systems even today display (un-)intentional bias.</p>
<p>Like many good ideas that pop into my mind, I had pushed it in the &ldquo;sounds fun, may attempt something on it later&rdquo; shelf in my mind closet. It stayed there until the first year of my PhD when my advisor asked me to look into this problem.</p>
<h2 id="the-problem-setup">The Problem Setup</h2>
<p>I considered a supervised learning setup in this problem. This meant that I assumed a dataset $\mathcal{D}$ consisting of a 3-tuple $(\mathbf{x}_i, y_i, s_i)$. The first two are pretty standard - a n-dimensional data point and the associated label. The third component is interesting. In fairness literature this is is called the <em>sensitive attribute</em>.</p>
<p>Imagine you&rsquo;re a banker whose main job is to either approve or deny a loan to an individual. You may look at the person&rsquo;s age, gender, income, race, education and based on the available data (in your Excel sheet) you decide if this person is worth the risk. Now, as a human you can let your bias creep in the process but the flip side is that you will be held accountable in case the denied person decides to sue you.</p>
<p>On the other hand, if you offload the task to a machine with &ldquo;advanced&rdquo; AI algorithms, no one can be held accountable since the outcome is determined by the machine based on countless comparisons from previous cases. You can simply shrug and walk away. There&rsquo;s a catch however - the &ldquo;AI&rdquo; that you used may have been trained on a dataset that encoded some sort of biases in the training set itself! For example, individuals belonging to certain age or gender were labeled as denied more frequently than others. This particular feature in data is called a <em>sensitive feature</em> or <em>sensitive attribute</em> since it biases a machine learning algorithm&rsquo;s output on some features which inherently must be ignored by an impartial human.</p>
<p>When I read through the literature on this topic, something bothered me for days. One day, it clicked. The problem with defining this $s_i$ beforehand and optimizing a machine learning algorithm to remove dependence on $s_i$ is like telling a child to close it&rsquo;s ears when it hears a particular phrase. However, children are intelligent and can often guess what you&rsquo;re gonna say based on context and past data. Similarly, machine learning algorithms will learn <em>associations</em> between features and still secretly bias their decisions on the sensitive feature. Reading more and more into this, I had two insights:</p>
<ol>
<li>Not all features (including $s_i$) contribute equally to the model&rsquo;s output.</li>
<li>Removing dependence on $s_i$ for decision making is not foolproof - the model may use <em>proxy</em> sensitive features to make predicitions.</li>
</ol>
<h2 id="from-insights-to-algorithmic-components">From insights to algorithmic components</h2>
<p>The first insight implies that there exist a subset of features in data that contribute most towards the overall performance of the model. I called  this subset a <em>critical feature set</em>.</p>
<p>How do we find such a set? We clearly cannot assume it from examining the data. My solution was to run a pre-processing step on the training set:</p>
<ul>
<li>
<p>First, I define a metric $\mathcal{M}$ that I measure to evaluate the model&rsquo;s performance. For different tasks, these can be accuracy, mean squared error or F1 score.</p>
</li>
<li>
<p>Partition the training data into $K$ non overlapping folds. Then train the model on $K-1$ fold and evaluate the performance on the remaining fold.</p>
</li>
<li>
<p>Run the procedure $N$ times with feature permutation (this prevents the machine learning model from &ldquo;cheating&rdquo; by memorization).</p>
</li>
</ul>
<p>At the end of the procedure, I would have built a &ldquo;feature importance vector&rdquo; $\mathcal{I}_x$ where $f_i \in \mathcal{I}_x$:</p>
<p>$$
f_i  = \Delta - \frac{1}{N}\sum_{i}^N \delta_i
$$</p>
<p>Here $\Delta$ is the metric value when no features are permuted and $\delta_i$ is the metric value when the $i^{th}$ permutation is applied to the data.</p>
<p>The second insight implied that the input sensitive variable $s_i$ may have correlations with other features within the data. Thus, as a second pre-processing step, I estimate $\Sigma$ by fitting a maximum likelihood estimator on the training data.</p>
<h2 id="testing-the-insights-on-real-data">Testing the insights on real data</h2>
<p>To evaluate these insights on real world datasets, I selected two representative datasets. The first one called ADULT is a collection of people described by various features like age, race, gender, education etc. The task is to determine if they are good candidate for a loan. The second one is the famous COMPAS dataset that accompanied the story about how a machine learning decision system was more likely to re-incarcerate African-Americans than white Americans. The sensitive feature in ADULT dataset is the person&rsquo;s gender while in COMPAS it&rsquo;s race of the individual.</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/fairness/adult_crit_feats.png"
        data-srcset="/fairness/adult_crit_feats.png, /fairness/adult_crit_feats.png 1.5x, /fairness/adult_crit_feats.png 2x"
        data-sizes="auto"
        alt="/fairness/adult_crit_feats.png"
        title="alt text" />
<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/fairness/compas_crit_feats.png"
        data-srcset="/fairness/compas_crit_feats.png, /fairness/compas_crit_feats.png 1.5x, /fairness/compas_crit_feats.png 2x"
        data-sizes="auto"
        alt="/fairness/compas_crit_feats.png"
        title="alt text" /></p>
<p>The outcome of computing feature importance is very interesting. You can see for yourself that neither gender nor race have a significant effect on the prediction of the machine learning systems <em>on their own</em>. Thus, if they are providing biased results it must be because of these features being correlated with more predicitve ones.</p>
<h2 id="measures-of-fairness">Measures of Fairness</h2>
<p>For a supervised learning paradigm, we define three distinct variables. $\hat{Y}$ is the predicted variable by a supervised learning algorithm, $\mathcal{Y}$ is the ground truth label and $\mathcal{S}$ is the sensitive feature of the data point.</p>
<p>Now, if we do have access to the ground truth labels, we can evaluate the fairness by conditioning the outcome on $\mathcal{Y}$. Some criteria that fall in this category are Equal Odds (where $D = -1 \perp \mathcal{S} | \mathcal{Y} = 1$ and $D = 1 \perp \mathcal{S} | \mathcal{Y}=1$)<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, and Equal Opportunity (where $D = 1 \perp \mathcal{S} | Y=1$)<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>. A defining characteristic of these metrics is that they  can be interpreted as a score function $\mathcal{L}: \mathcal{X} \times \mathcal{S} \times \mathcal{Y} \rightarrow \mathbb{R}$. For instance, the Equal Opportunity metric can be expressed as</p>
<p>[
\begin{equation}
\mathcal{L}(\mathcal{X}, \mathcal{Y},  \mathcal{S}) =  \mathbb{E}\left[|l^{+}<em>{a}(f(\textbf{x}</em>{i}), y_{i}) - l^{+}<em>{b}(f(\textbf{x}</em>{i}), y_{i})|\right]
\end{equation}
]</p>
<p>and a corresponding score function for Equality of True Negative Rates can be written as</p>
<p>$$
\mathcal{L}(\mathcal{X}, \mathcal{Y},  \mathcal{S}) =  \mathbb{E}[|l^{-}<em>{a}(f(\textbf{x}</em>{i}), y_{i}) - l^{-}<em>{b}(f(\textbf{x}</em>{i}), y_{i})|]
$$</p>
<p>Here, $l^{\pm}_{g}$ is the per sample loss for any learning algorithm $f$ on either positively or negatively labeled points belonging to a sensitive group $g$.</p>
<p>It is nice and easy to have metrics defined in terms of ground truth. However, there are cases when we can only observe bias in a system by observing the predicted outcome. For example, a facial scanning system that predicts if you&rsquo;re a criminal or not generally doesn&rsquo;t provide access to the training data of mugshots it was trained on. Hence, we need metrics that can evaluate fairness based on predicted outcomes $\hat{Y}$. Some examples of metrics belonging to this class are Equality of Negative Predictive Value (def: $Y = -1 \perp \mathcal{S} | \hat{Y}=-1$) and Equality of Positive Predictive Value (def:$\mathcal{Y} = 1 \perp \mathcal{S} | \hat{Y}=1$). In other words, if a outcome was negative then it should correspond to a negative groundtruth label irrespective of the value of $S$.</p>
<h2 id="modeling-constraints-within-machine-learning-algorithms">Modeling constraints within machine learning algorithms</h2>
<p>Now that we know <em>what</em> metrics are used to define fairness, we are in a good position to try and train a machine learning algorithm to respect these metrics for making fair-er predictions.</p>
<p>Let&rsquo;s take the equal oppportunity metric as an example. The metric is defined as $P(\hat{Y}=1 | S=a, Y=1) - P(\hat{Y}=1 | S=b, Y=1) \leq \epsilon$. Here $\hat{Y}$ is the predicted value given an input datapoint i.e. $\hat{Y} = f(\mathbf{x})$. Here $\epsilon$ is the degree of unfairness we&rsquo;re willing to tolerate in the system. Prior to my work<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> it was standard to set $\epsilon = 0$.</p>
<p>However, I realized that we cannot make perfectly fair systems without damaging the performance of the model irreparably (and if we could achieve perfect fairness then the world would somehow become utopia).  Hence, I made a simple modification  to allow a <em>user</em> to determine how much fairness they want in their system. Hence, my formulation accepts $\epsilon = f_{tol}$ where $f_{tol}$ is user provided.</p>
<p>Let&rsquo;s revisit the loss $L(\mathcal{X}, \mathcal{Y}, S)$ defined above. The general idea in introducing fairness into the system is to somehow integrate the constraint $L(\mathcal{X}, \mathcal{Y}, S) \leq \epsilon$ where $\epsilon$ is either user defined or zero.</p>
<p>Linear models of machine learning model $f(\mathbf{x})= \langle \mathbf{w}, \phi(\mathbf{x})\rangle$ where $\phi(\mathbf{x})$ is some feature mapping and $\langle x, y\rangle = x^Ty$ a.k.a inner product. The weight vector $\mathbf{w}$ is essentially a separating hyperplane that can classify a data point into distinct categories. The simplest case of feature mapping is $\phi(\mathbf{x}) = \mathbf{x}$. There&rsquo;s a cool &ldquo;trick&rdquo; in machine learning that allows us to compute a high dimensional mapping from a given dataset. Why high dimensional? Because it may so happen that our dataset is non separable by $\mathbf{w}$ in the given number of dimensions. However, in some high dimensional space, it may be linearly separable. This &ldquo;trick&rdquo; is the kernel-trick that allows us to compute $\langle \phi(\mathbf{x}), \phi(\mathbf{x&rsquo;})\rangle$ by simply computing the kernel matrix $K(\mathbf{x}, \mathbf{x&rsquo;})$ for $\mathbf{x}, \mathbf{x&rsquo;} \in \mathcal{X}$. The advantage of defining a kernel like this is that we don&rsquo;t really need to worry about finding a &ldquo;good enough&rdquo; feature mapping. A famous example of kernel is the RBF Kernel $K(\mathbf{x}, \mathbf{x}&rsquo;) = e^{-\gamma ||\mathbf{x} - \mathbf{x&rsquo;}||_2}$.</p>
<p>One of the best linear models to use in supervised learning is the Support Vector Machine (SVM). The unconstrained objective function can be written as:</p>
<p>$$
\min_{\mathbf{w}} \sum_{i=1}^n l(f(\mathbf{x}), y) + \lambda ||\mathbf{w}||_2
$$</p>
<p>Where $\lambda ||\mathbf{w}||_2$ is the regularization constraint on the weight vector so that it&rsquo;s constrained within a ball of radius $\lambda$. But how do we integrate a fairness constraint like Equal Opportunity in training a linear model? We know we need to constrain the weight vector $\mathbf{w}$ to disregard the sensitive variable somehow. Donini <em>et al.</em><sup id="fnref1:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> propose a constraint of the form $\langle \mathbf{w}, \mathbf{u} \rangle \leq \epsilon$ where $\mathbf{u}$ is the centroid of positively labeled points belonging to a particular sensitive group. So if $\mathcal{S} = {a, b}$:</p>
<p>$$
\mathbf{u}_a = \frac{1}{N^+<em>a}\sum</em>{j=1}^n \phi(\mathbf{x}_j)
$$</p>
<p>Then to draw a hyperplane that strictly respects the EO metric, we need to have the weight vector <em>orthogonal</em> to $\mathbf{u}_a - \mathbf{u}_b$. In other words $\langle \mathbf{w}, \mathbf{u}_a - \mathbf{u}_b\rangle = 0$. So the modification to the SVM objective is:</p>
<p>$$
\min_{\mathbf{w}} \sum_{i=1}^n l(f(\mathbf{x}), y) + \lambda ||\mathbf{w}||_2\  s.t.  \langle \mathbf{w}, \mathbf{u}_a - \mathbf{u}_b\rangle = 0
$$</p>
<p>To integrate the user defined fairness tolerane $f_{tol}$ we can adjust this constraint to be  $\langle \mathbf{w}, \mathbf{u}_a - \mathbf{u}<em>b\rangle \leq f</em>{tol}$.</p>
<p>One can then setup the dual of the constrained optimization problem and solve for the support vector coefficients $\mathbf{\alpha}$ that respect the constraint. The package <a href="https://github.com/aicaffeinelife/CONFAIR/tree/master" target="_blank" rel="noopener noreffer ">CONFAIR</a> implements all code for fairness evaluation using SVMs. It supports $f_{tol} = 0$ and $f_{tol} \neq 0$ cases.</p>
<h2 id="how-much-fairness-is-enough">How much fairness is enough?</h2>
<p>The literature I read at that time suggested we needed absolutely good performance on the given fairness metric. If we step back and look at the law of conservation, it will become apparent that a gain in some quantity must come at the expense of another. In this case the predictive accuracy is the obvious victim. The other victim is more subtle - by increasing fairness on one metric by strict constraints, we may end up being unfair in the <em>opposite</em> direction. For example, by ensuring the positive predictions do not take the sensitive variable into account we may condition the negative prediction on the said sensitive variable.</p>
<p>To measure this I optimized the SVM with $f_{tol} = 0$ and $f_{tol} \neq 0$ and measured the Difference in Equal Opportunity (DEO) along with the NPV of the trained SVM on the test set. This is what I got:</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/fairness/confair_perforrmance_table.png"
        data-srcset="/fairness/confair_perforrmance_table.png, /fairness/confair_perforrmance_table.png 1.5x, /fairness/confair_perforrmance_table.png 2x"
        data-sizes="auto"
        alt="/fairness/confair_perforrmance_table.png"
        title="alt text" /></p>
<p>Here FERM is the case when $f_{tol} = 0$ and FairSVM is trained on full feature set with $f_{tol} \neq 0$. You can see that when choosing  the right set of features and optimizing less strictly, we can get better performance on both explicit and implicit fairness objectives. I also benchmarked the effect of varying $f_{tol}$ on various cases as well:</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="/fairness/confair_ftol_perf.png"
        data-srcset="/fairness/confair_ftol_perf.png, /fairness/confair_ftol_perf.png 1.5x, /fairness/confair_ftol_perf.png 2x"
        data-sizes="auto"
        alt="/fairness/confair_ftol_perf.png"
        title="alt text" /></p>
<p>We can see that different values of $f_{tol}$ produce different amounts of fairness in different datasets. Thus, the tolerance is a highly data-dependent hyperparameter and can be chosen depending on the needs of the algorithm designer.</p>
<h2 id="conclusion">Conclusion</h2>
<p>As we enter into an era where AI models are ubiquitous, we need to be especially careful about the inherent bias that these models absorb when trained on biased data created by us. This blog post discusses some ways in which fairness can be included in a given AI model. The model may itself be outdated by today&rsquo;s standard but the concepts of sensitive variables, critical features and fairness metrics are still relevant today. I would argue they are more than critical even since AI models will soon be deployed in decision making systems that will have a large impact on real people&rsquo;s lives.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>M.Hardt, E.Price &ldquo;Equality of Opportunity in Supervised Learning&rdquo;&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>M. Donini, L. Oneto, S. Ben-David, J. Shawe-Taylor, and M. Pontil, “Empirical Risk Minimization under Fairness Constraints&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>M. B. Zafar, I. Valera, M. G. Rodriguez, and K. P. Gummadi, “Fairness Beyond Disparate Treatment &amp; Disparate Impact: Learning Classification without
Disparate Mistreatment,”&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>A.Kulshrestha, I.Safro, &ldquo;CONFAIR : Interpretable and Configurable Algorithmic Fairness&rdquo;&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 2025-07-23</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/" data-title="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms" data-via="aicaffeinelife"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/" data-title="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/" data-title="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="https://aicaffeinelife.github.io/posts/2025_07_23_fairlearn/" data-title="In all fairness: Engineering Fairness in Modern Machine Learning Algorithms"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/2024_06_07_classical_shadows/" class="prev" rel="prev" title="The shadowy art of classical shadows - Part 2"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>The shadowy art of classical shadows - Part 2</a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.119.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2025</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="akulsh912.com" target="_blank">Ankit Kulshrestha</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
